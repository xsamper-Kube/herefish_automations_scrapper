{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08bd6f0b",
   "metadata": {},
   "source": [
    "# Bullhorn Automation Scraper\n",
    "This notebook automates the extraction of Bullhorn (Herefish) automation details using Selenium.\n",
    "\n",
    "#### **Features**\n",
    "* Hibernation Check: Automatically scans your \"Hibernated\" table first to flag inactive automations in the final report.\n",
    "* Progress Backups: Saves results to Excel every 50 records to prevent data loss.\n",
    "* Fault Tolerance: * InvalidSessionId Management: Specifically catches session crashes to prevent the notebook from hanging.\n",
    "    * Auto-Recovery: If a session expires or drops, the script automatically re-logs in and resumes exactly where it left off.\n",
    "    * Anti-Blank Loading: If no content appears after 15 seconds, the script will automatically reload the page until the data is visible.\n",
    "\n",
    "#### **Setup Requirements**\n",
    "* Credentials: Enter your login details in the USERNAME and PASSWORD variables in Cell 2.\n",
    "* Browser: Google Chrome must be installed on your computer.\n",
    "* Speed: Extraction takes approximately ~25 minutes per 1,000 automations.\n",
    "\n",
    "#### **Final Output**\n",
    "* The scraper generates an Excel file with two sheets:\n",
    "* Automations: Detailed data for the requested ID range, including a TRUE/FALSE column for hibernation status.\n",
    "* Hibernated Automations: A full list of all currently hibernated automations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd38a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, InvalidSessionIdException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials and configuration\n",
    "USERNAME = \"YOUR_USERNAME_HERE\"\n",
    "PASSWORD = \"YOUR_PASSWORD_HERE\"\n",
    "\n",
    "# XPATHs to find elements\n",
    "## Automation Credentials XPATHs\n",
    "EMAIL_XPATH = \"/html/body/div/div/div[1]/div/form/div[2]/input\"\n",
    "PASS_XPATH = \"/html/body/div/div/div[1]/div/form/div[3]/input\"\n",
    "BTN_XPATH = \"/html/body/div/div/div[1]/div/form/div[5]/button\"\n",
    "## Hibernate scraper XPATHs\n",
    "HIBERNATED_XPATH = \"/html/body/div[2]/div[1]/div/div[3]/div/div[3]/div[1]/span[1]\"\n",
    "HIBERNATED_TABLE_XPATH = \"/html/body/div[2]/div[1]/div/div[3]/div/div[3]/div[2]/div[1]/table\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60752fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: The Login Function\n",
    "# This function allows us to trigger a login at the start or whenever the session drops.\n",
    "\n",
    "def login_to_herefish(driver):\n",
    "    print(\"Initiating login sequence...\")\n",
    "    driver.get(\"https://app.herefish.com/\")\n",
    "    time.sleep(5) \n",
    "    \n",
    "    try:\n",
    "        # Check if we are already logged in or need to enter credentials\n",
    "        email_input = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, EMAIL_XPATH)))\n",
    "        email_input.send_keys(USERNAME)\n",
    "        \n",
    "        password_input = driver.find_element(By.XPATH, PASS_XPATH)\n",
    "        password_input.send_keys(PASSWORD)\n",
    "        \n",
    "        login_button = driver.find_element(By.XPATH, BTN_XPATH)\n",
    "        login_button.click()\n",
    "        \n",
    "        time.sleep(5)  # Allow time for login to process\n",
    "\n",
    "        # Wait until we are redirected away from the login page\n",
    "        driver.get(\"https://app.herefish.com/Automations/Automations\")\n",
    "        WebDriverWait(driver, 15).until(EC.url_changes(\"https://app.herefish.com/\"))\n",
    "        print(\"Login successful.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6920fb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login sequence...\n",
      "Login successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xavier Samper\\AppData\\Local\\Temp\\ipykernel_17432\\2242538242.py:18: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  hibernated_df = pd.read_html(table_element.get_attribute('outerHTML'))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hibernated Automations extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: New Session, Exact Login Function, Targeted Click, extract Hibernated Table\n",
    "\n",
    "# 1. Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# 2. Use the existing login function to authenticate\n",
    "login_to_herefish(driver)\n",
    "\n",
    "# 3. After login, navigate to the Automations section\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, HIBERNATED_XPATH))).click()\n",
    "\n",
    "# Locate the table using the XPATH and extract its HTML\n",
    "time.sleep(2)                                                           # Allow time for table to fully render\n",
    "table_element = driver.find_element(By.XPATH, HIBERNATED_TABLE_XPATH)\n",
    "\n",
    "# 4. Use pandas to read the HTML table into a DataFrame\n",
    "hibernated_df = pd.read_html(table_element.get_attribute('outerHTML'))[0]\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# 5. Create a set from the 'Automation Name' column for O(1) lookup speed. This will greatly speed up the \"in\" checks later.\n",
    "hibernated_ids_set = set(hibernated_df['Automation Name'].astype(str).tolist())\n",
    "\n",
    "print(\"Hibernated Automations extracted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8486ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating login sequence...\n",
      "Login successful.\n",
      "--- Scraper Estimation ---\n",
      "Total IDs: 31\n",
      "Estimated duration: 0 minutes, 46 seconds\n",
      "Expected to finish at: 11:15 AM on December 24, 2025\n",
      "--------------------------\n",
      "ID 7841: Skipped: Automation not found or deleted\n",
      "ID 7842: Scraped successfully.\n",
      "ID 7843: Skipped: Automation not found or deleted\n",
      "ID 7844: Skipped: Automation not found or deleted\n",
      "ID 7845: Scraped successfully.\n",
      "ID 7846: Scraped successfully.\n",
      "ID 7847: Scraped successfully.\n",
      "ID 7848: Skipped: Automation not found or deleted\n",
      "ID 7849: Skipped: Automation not found or deleted\n",
      "ID 7850: Skipped: Automation not found or deleted\n",
      "ID 7851: Skipped: Automation not found or deleted\n",
      "ID 7852: Skipped: Automation not found or deleted\n",
      "ID 7853: Skipped: Automation not found or deleted\n",
      "ID 7854: Skipped: Automation not found or deleted\n",
      "ID 7855: Skipped: Automation not found or deleted\n",
      "ID 7856: Skipped: Automation not found or deleted\n",
      "ID 7857: Skipped: Automation not found or deleted\n",
      "ID 7858: Skipped: Automation not found or deleted\n",
      "ID 7859: Skipped: Automation not found or deleted\n",
      "ID 7860: Skipped: Automation not found or deleted\n",
      "ID 7861: Skipped: Automation not found or deleted\n",
      "ID 7862: Skipped: Automation not found or deleted\n",
      "ID 7863: Skipped: Automation not found or deleted\n",
      "ID 7864: Skipped: Automation not found or deleted\n",
      "ID 7865: Skipped: Automation not found or deleted\n",
      "ID 7866: Skipped: Automation not found or deleted\n",
      "ID 7867: Skipped: Automation not found or deleted\n",
      "ID 7868: Skipped: Automation not found or deleted\n",
      "ID 7869: Skipped: Automation not found or deleted\n",
      "ID 7870: Skipped: Automation not found or deleted\n",
      "ID 7871: Scraped successfully.\n",
      "\n",
      "--- Scraping Complete. Kept 5 records ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: The Main Scraper Loop with Automatic Retries\n",
    "# This version will not skip an ID if it hits a timeout; it will retry until success.\n",
    "\n",
    "# 1. Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# 2. Initial Login\n",
    "login_to_herefish(driver)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "POPUP_XPATH = \"//div[contains(@class, 'modal-body')]//div[contains(text(), 'not found')]\"       # Popup when there isn't any automation\n",
    "CONTENT_XPATH = \"/html/body/div[2]/div[1]/div/div[6]\"                                           # If there is content, it appears here\n",
    "\n",
    "\n",
    "wait = WebDriverWait(driver, 15)\n",
    "automation_ids = list(range(7841, 7872)) \n",
    "scraped_data = []\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Print how long will it take approximately\n",
    "# 2. Calculate the estimated duration in minutes\n",
    "num_ids = len(automation_ids)\n",
    "est_total_minutes = (num_ids / 1000) * 25\n",
    "\n",
    "# 3. Calculate expected end time\n",
    "now = datetime.now()\n",
    "expected_end_time = now + timedelta(minutes=est_total_minutes)\n",
    "\n",
    "# 4. Format the duration (Hours, Minutes)\n",
    "hours, minutes = divmod(int(est_total_minutes), 60)\n",
    "\n",
    "# 5. Output the results\n",
    "print(f\"--- Scraper Estimation ---\")\n",
    "print(f\"Total IDs: {num_ids}\")\n",
    "\n",
    "# Duration print logic\n",
    "if hours > 0:\n",
    "    print(f\"Estimated duration: {hours} hours, {minutes} minutes\")\n",
    "else:\n",
    "    seconds = int((est_total_minutes % 1) * 60)\n",
    "    print(f\"Estimated duration: {minutes} minutes, {seconds} seconds\")\n",
    "\n",
    "# Expected completion time and date\n",
    "print(f\"Expected to finish at: {expected_end_time.strftime('%I:%M %p on %B %d, %Y')}\")\n",
    "print(f\"--------------------------\")\n",
    "\n",
    "\n",
    "# --- MAIN LOOP WITH RETRIES ---\n",
    "index = 0\n",
    "while index < len(automation_ids):\n",
    "    automation_id = automation_ids[index]\n",
    "    url = f\"https://app.herefish.com/Automations/Automation/{automation_id}\"\n",
    "    \n",
    "    # --- REDIRECT/SESSION CHECK ---\n",
    "    if driver.current_url in [\"https://app.herefish.com/\", \"https://app.herefish.com/Login\", \"https://app.herefish.com/Account/Login\"]:\n",
    "        print(f\"Session lost at ID {automation_id}. Re-logging...\")\n",
    "        login_to_herefish(driver)\n",
    "        # We don't increment index, so it retries this ID after login\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(1)  # Initial wait for page load\n",
    "\n",
    "    automation_name = None\n",
    "    status = None\n",
    "    content_text = None\n",
    "    error_note = None\n",
    "    success = False # Track if we should move to the next ID\n",
    "\n",
    "    try:\n",
    "        # Check for redirect immediately after navigation\n",
    "        if \"Login\" in driver.current_url or driver.current_url == \"https://app.herefish.com/\":\n",
    "            print(f\"ID {automation_id}: Redirected to login. Retrying...\")\n",
    "            continue \n",
    "\n",
    "        # Wait for the URL to reflect the correct ID\n",
    "        wait.until(EC.url_contains(str(automation_id)))\n",
    "\n",
    "        found_state = None\n",
    "        # Poll for either the 'Not Found' popup or the actual content\n",
    "        for _ in range(15): \n",
    "            popups = driver.find_elements(By.XPATH, POPUP_XPATH)\n",
    "            if popups and popups[0].is_displayed():\n",
    "                found_state = \"popup\"\n",
    "                break\n",
    "            \n",
    "            content_elements = driver.find_elements(By.XPATH, CONTENT_XPATH)\n",
    "            if content_elements and len(content_elements[0].text.strip()) > 0:\n",
    "                found_state = \"content\"\n",
    "                break\n",
    "            \n",
    "            time.sleep(1)       # Wait a bit before re-checking\n",
    "\n",
    "        if found_state == \"popup\":\n",
    "            print(f\"ID {automation_id}: Skipped: Automation not found or deleted\")\n",
    "            # We found a definitive answer (it doesn't exist), so move to next ID\n",
    "            success = True \n",
    "\n",
    "        elif found_state == \"content\":\n",
    "            title_container = driver.find_element(By.CLASS_NAME, \"title\")\n",
    "            automation_name = driver.execute_script(\n",
    "                \"return arguments[0].childNodes[0].textContent.trim();\", \n",
    "                title_container\n",
    "            )\n",
    "            status_labels = title_container.find_elements(By.CSS_SELECTOR, \"span.label:not(.ng-hide)\")\n",
    "            status = status_labels[0].text.strip() if status_labels else \"N/A\"\n",
    "            content_text = driver.find_element(By.XPATH, CONTENT_XPATH).text\n",
    "            \n",
    "            print(f\"ID {automation_id}: Scraped successfully.\")\n",
    "            success = True # Scrape finished, move to next ID\n",
    "\n",
    "        else:\n",
    "            # This is the Timeout / Blank page scenario\n",
    "            print(f\"ID {automation_id}: Timeout/Blank. Retrying same ID...\")\n",
    "            time.sleep(2) # Short breather before retry\n",
    "            continue # This restarts the loop WITHOUT incrementing index\n",
    "\n",
    "    except Exception as e:\n",
    "        if isinstance(e, InvalidSessionIdException):\n",
    "            print(\"Session crashed. Attempting to restart driver...\")\n",
    "            driver.quit()\n",
    "            driver = webdriver.Chrome()\n",
    "            login_to_herefish(driver)\n",
    "            continue\n",
    "            \n",
    "        print(f\"ID {automation_id}: Error {type(e).__name__}. Retrying...\")\n",
    "        continue\n",
    "\n",
    "    # Only save data and move to the next ID if we actually got a result (Content or Popup)\n",
    "    if success:\n",
    "        if found_state == \"content\":\n",
    "            # Check if the scraped name exists in our hibernated set\n",
    "            is_hibernated = automation_name in hibernated_ids_set\n",
    "            \n",
    "            scraped_data.append({\n",
    "                \"Automation_ID\": automation_id,\n",
    "                \"Name\": automation_name,\n",
    "                \"Status\": status,\n",
    "                \"Is_Hibernated\": is_hibernated,  # New TRUE/FALSE column\n",
    "                \"Content\": content_text,\n",
    "                \"Extraction_Notes\": \"Scraped\"\n",
    "            })\n",
    "\n",
    "        # Periodic save every 2 IDs\n",
    "        if len(scraped_data) > 0 and len(scraped_data) % 2 == 0:\n",
    "            file_path = f\"automation_results_{today}.xlsx\"\n",
    "            with pd.ExcelWriter(file_path) as writer:\n",
    "                # Save current progress to 'Automations' sheet\n",
    "                pd.DataFrame(scraped_data).to_excel(writer, sheet_name='Automations', index=False)\n",
    "                \n",
    "                # Save the hibernated list (from Cell 4) to its own sheet\n",
    "                hibernated_df.to_excel(writer, sheet_name='Hibernated Automations', index=False)\n",
    "            \n",
    "\n",
    "        \n",
    "        index += 1 # Move to the next ID in the list\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Define the filename once\n",
    "file_path = f\"automation_results_{today}.xlsx\"\n",
    "\n",
    "# Use the ExcelWriter to \"hold open\" the file while you write multiple sheets\n",
    "with pd.ExcelWriter(file_path) as writer:\n",
    "    # Write the main results\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "    df.to_excel(writer, sheet_name='Automations', index=False)\n",
    "    # Write the hibernated list\n",
    "    hibernated_df.to_excel(writer, sheet_name='Hibernated Automations', index=False)\n",
    "\n",
    "\n",
    "print(f\"\\n--- Scraping Complete. Kept {len(df)} records ---\")\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
